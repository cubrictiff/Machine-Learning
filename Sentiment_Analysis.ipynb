{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1jfMQGD-4N-cdBuXqdwfWjmnRJMvhv7tb",
      "authorship_tag": "ABX9TyNCJVHHA9mJ6nCuBOUe9A3x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cubrictiff/Machine-Learning/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9717KBPmErj",
        "colab_type": "text"
      },
      "source": [
        "**Data Exploring**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV6GIn0Zpy5t",
        "colab_type": "code",
        "outputId": "ce96afc7-45d5-4083-84b4-c9cde4142322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "with open('drive/My Drive/Sentiment Analysis/train.tsv') as doc:\n",
        "    rd = csv.reader(doc, delimiter=\"\\t\", quotechar='\"')\n",
        "    col = ['PhraseId','SentenceId','Phrase','Sentiment']\n",
        "    df = pd.DataFrame(rd, columns = col)\n",
        "df = df.iloc[1:]\n",
        "df = df.reset_index()\n",
        "df "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156055</th>\n",
              "      <td>156056</td>\n",
              "      <td>156056</td>\n",
              "      <td>8544</td>\n",
              "      <td>Hearst 's</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156056</th>\n",
              "      <td>156057</td>\n",
              "      <td>156057</td>\n",
              "      <td>8544</td>\n",
              "      <td>forced avuncular chortles</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156057</th>\n",
              "      <td>156058</td>\n",
              "      <td>156058</td>\n",
              "      <td>8544</td>\n",
              "      <td>avuncular chortles</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156058</th>\n",
              "      <td>156059</td>\n",
              "      <td>156059</td>\n",
              "      <td>8544</td>\n",
              "      <td>avuncular</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156059</th>\n",
              "      <td>156060</td>\n",
              "      <td>156060</td>\n",
              "      <td>8544</td>\n",
              "      <td>chortles</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>156060 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         index  ... Sentiment\n",
              "0            1  ...         1\n",
              "1            2  ...         2\n",
              "2            3  ...         2\n",
              "3            4  ...         2\n",
              "4            5  ...         2\n",
              "...        ...  ...       ...\n",
              "156055  156056  ...         2\n",
              "156056  156057  ...         1\n",
              "156057  156058  ...         3\n",
              "156058  156059  ...         2\n",
              "156059  156060  ...         2\n",
              "\n",
              "[156060 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJgNS2Rep7AU",
        "colab_type": "code",
        "outputId": "20c5a881-8332-4edb-d186-bbaa7167be2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "phrase = df['Phrase']\n",
        "phrase[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfpH7Mv8rNLl",
        "colab_type": "code",
        "outputId": "5921aa27-e8ad-4bda-f554-55b672ee0b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "import seaborn as sns\n",
        "sent = df.groupby(['Sentiment']).size()\n",
        "sns.barplot(sent.keys(),sent.values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment\n",
            "0     7072\n",
            "1    27273\n",
            "2    79582\n",
            "3    32927\n",
            "4     9206\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9cb00665c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Owb8UqrP14",
        "colab_type": "code",
        "outputId": "5ecb40b3-62de-4b19-fc05-5ee8fa45a005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "phrase[0].split()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'series',\n",
              " 'of',\n",
              " 'escapades',\n",
              " 'demonstrating',\n",
              " 'the',\n",
              " 'adage',\n",
              " 'that',\n",
              " 'what',\n",
              " 'is',\n",
              " 'good',\n",
              " 'for',\n",
              " 'the',\n",
              " 'goose',\n",
              " 'is',\n",
              " 'also',\n",
              " 'good',\n",
              " 'for',\n",
              " 'the',\n",
              " 'gander',\n",
              " ',',\n",
              " 'some',\n",
              " 'of',\n",
              " 'which',\n",
              " 'occasionally',\n",
              " 'amuses',\n",
              " 'but',\n",
              " 'none',\n",
              " 'of',\n",
              " 'which',\n",
              " 'amounts',\n",
              " 'to',\n",
              " 'much',\n",
              " 'of',\n",
              " 'a',\n",
              " 'story',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzRDec48mWsZ",
        "colab_type": "text"
      },
      "source": [
        "**Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yo9lC0NrTlU",
        "colab_type": "code",
        "outputId": "25f18593-f815-4818-ea55-b13e695f563c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "  \n",
        "\n",
        "\n",
        "def to_lowercase(words):\n",
        "    word_lst = []\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        word_lst.append(word)\n",
        "    return word_lst\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    word_lst = []\n",
        "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "    for word in words:\n",
        "        word = word.translate(table)\n",
        "        word_lst.append(word)\n",
        "    word_lst = [x for x in word_lst if x]\n",
        "    return word_lst\n",
        "\n",
        "def remove_num(words):\n",
        "    word_lst = []\n",
        "    for word in words:\n",
        "        if not word.isdigit():\n",
        "            word_lst.append(word)\n",
        "    return word_lst\n",
        "\n",
        "def remove_stopword(words):\n",
        "    word_lst = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            word_lst.append(word)\n",
        "    return word_lst\n",
        "\n",
        "def stem_word(words):\n",
        "    stemmer = LancasterStemmer()\n",
        "    word_lst = []\n",
        "    for word in words:\n",
        "        word = stemmer.stem(word)\n",
        "        word_lst.append(word)\n",
        "    return word_lst\n",
        "\n",
        "def lemmatize_word(words):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    word_lst = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        word_lst.append(lemma)\n",
        "    return word_lst\n",
        "\n",
        "def normalize_word(words):\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_num(words)\n",
        "    words = remove_stopword(words)\n",
        "    words = stem_word(words)\n",
        "    words = lemmatize_word(words)\n",
        "    \n",
        "    return words\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv5t_1dorY8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Token'] = df['Phrase'].apply(nltk.word_tokenize) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjlodtpErlBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Token'] = df['Token'].apply(normalize_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHLYHzhJrqnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_set = set()\n",
        "for word in df['Token']:\n",
        "    for i in word:\n",
        "        word_set.add(i)\n",
        "word_dict = {word:i for i, word in enumerate(word_set,1)}\n",
        "\n",
        "df['Token'] = df['Token'].apply(lambda l: [word_dict[word] for word in l])\n",
        "df['Token']\n",
        "\n",
        "long_word = df['Token'].str.len().max()\n",
        "long_word\n",
        "\n",
        "all_token = np.array([i for i in df['Token']])\n",
        "df['Sentiment'] = df['Sentiment'].astype('int')\n",
        "label = np.array([i for i in df['Sentiment']])\n",
        "feature = np.zeros((len(all_token),long_word), dtype = int)\n",
        "\n",
        "for i,words in enumerate(df['Token']):\n",
        "    feature[i,:len(words)] = words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlliUv27sJ-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split = 0.8\n",
        "\n",
        "train_index = int(len(feature)*0.8)\n",
        "x_train, x_remain = feature[:train_index], feature[train_index:]\n",
        "y_train, y_remain = label[:train_index], label[train_index:]\n",
        "\n",
        "test_index = int(len(x_remain)*0.5)\n",
        "x_validate, x_test = x_remain[:test_index], x_remain[test_index:]\n",
        "y_validate, y_test = y_remain[:test_index], y_remain[test_index:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhugsccwsQmw",
        "colab_type": "code",
        "outputId": "7e8b7055-3d57-49bb-c3f1-8d71b56ab0e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(type(y_train[0]))\n",
        "print(x_test.shape)\n",
        "print(x_validate.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.int64'>\n",
            "(15606, 30)\n",
            "(15606, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn5JyjHzlzTG",
        "colab_type": "text"
      },
      "source": [
        "**Model** **Building** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfvQFsLVsR0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iljZkYfpwXZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
        "validate_data = TensorDataset(torch.from_numpy(x_validate), torch.from_numpy(y_validate))\n",
        "test_data = TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypRCPuunwcYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size  = 64\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "valid_loader = DataLoader(validate_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JGvJ489we8u",
        "colab_type": "code",
        "outputId": "4a762afb-d175-4143-d35e-437302ad931c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b0W968OwtZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size, output_size,embedding_dim,\n",
        "               hiddent_dim,n_layers,drop_prob=0.5):\n",
        "\n",
        "    super(SentimentRNN,self).__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    # embedding and LSTM layers\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim,hidden_dim,n_layers,\n",
        "                        dropout=drop_prob, batch_first = True)\n",
        "    \n",
        "    # dropout layer\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "    # linear\n",
        "    self.fc = nn.Linear(hidden_dim,output_size)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    embeds = self.embedding(x)\n",
        "\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "    lstm_out = lstm_out.transpose(0,1)\n",
        "    lstm_out = lstm_out[-1]\n",
        "\n",
        "    out = self.dropout(lstm_out)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    return out, hidden\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "            \n",
        "        return hidden\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQl8LieF0im_",
        "colab_type": "code",
        "outputId": "7734b6e0-77a1-4927-de57-bed7d589620b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "vocab_size = len(word_set) + 1\n",
        "output_size = 5\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(9891, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8hzYFyJ071C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=0.003\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyUW2ZwtnHff",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CTKSsE71Aot",
        "colab_type": "code",
        "outputId": "dd4d5178-c255-401f-d3ec-e0468101c709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# training params\n",
        "epochs = 3 \n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "               val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                \n",
        "               inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "               output, val_h = net(inputs, val_h)\n",
        "               val_loss = criterion(output, labels)\n",
        "\n",
        "               val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/3... Step: 100... Loss: 1.348261... Val Loss: 1.301404\n",
            "Epoch: 1/3... Step: 200... Loss: 1.275716... Val Loss: 1.303926\n",
            "Epoch: 1/3... Step: 300... Loss: 1.283561... Val Loss: 1.310805\n",
            "Epoch: 1/3... Step: 400... Loss: 1.319135... Val Loss: 1.303073\n",
            "Epoch: 1/3... Step: 500... Loss: 1.334146... Val Loss: 1.301840\n",
            "Epoch: 1/3... Step: 600... Loss: 1.136655... Val Loss: 1.303479\n",
            "Epoch: 1/3... Step: 700... Loss: 1.276804... Val Loss: 1.303246\n",
            "Epoch: 1/3... Step: 800... Loss: 1.238992... Val Loss: 1.268387\n",
            "Epoch: 1/3... Step: 900... Loss: 1.230065... Val Loss: 1.235794\n",
            "Epoch: 1/3... Step: 1000... Loss: 1.130429... Val Loss: 1.232271\n",
            "Epoch: 1/3... Step: 1100... Loss: 1.300996... Val Loss: 1.206121\n",
            "Epoch: 1/3... Step: 1200... Loss: 1.315115... Val Loss: 1.199940\n",
            "Epoch: 1/3... Step: 1300... Loss: 1.283123... Val Loss: 1.194853\n",
            "Epoch: 1/3... Step: 1400... Loss: 1.106670... Val Loss: 1.184140\n",
            "Epoch: 1/3... Step: 1500... Loss: 1.073022... Val Loss: 1.170003\n",
            "Epoch: 1/3... Step: 1600... Loss: 1.100027... Val Loss: 1.168924\n",
            "Epoch: 1/3... Step: 1700... Loss: 1.134367... Val Loss: 1.155329\n",
            "Epoch: 1/3... Step: 1800... Loss: 1.095074... Val Loss: 1.149268\n",
            "Epoch: 1/3... Step: 1900... Loss: 1.239671... Val Loss: 1.134800\n",
            "Epoch: 2/3... Step: 2000... Loss: 1.042550... Val Loss: 1.144987\n",
            "Epoch: 2/3... Step: 2100... Loss: 1.032059... Val Loss: 1.114984\n",
            "Epoch: 2/3... Step: 2200... Loss: 1.065051... Val Loss: 1.111217\n",
            "Epoch: 2/3... Step: 2300... Loss: 0.932156... Val Loss: 1.087518\n",
            "Epoch: 2/3... Step: 2400... Loss: 1.015770... Val Loss: 1.086526\n",
            "Epoch: 2/3... Step: 2500... Loss: 0.979338... Val Loss: 1.093912\n",
            "Epoch: 2/3... Step: 2600... Loss: 1.033871... Val Loss: 1.099299\n",
            "Epoch: 2/3... Step: 2700... Loss: 1.076831... Val Loss: 1.085076\n",
            "Epoch: 2/3... Step: 2800... Loss: 1.073469... Val Loss: 1.061725\n",
            "Epoch: 2/3... Step: 2900... Loss: 1.071873... Val Loss: 1.062373\n",
            "Epoch: 2/3... Step: 3000... Loss: 1.055575... Val Loss: 1.081888\n",
            "Epoch: 2/3... Step: 3100... Loss: 1.030248... Val Loss: 1.049351\n",
            "Epoch: 2/3... Step: 3200... Loss: 0.977376... Val Loss: 1.068428\n",
            "Epoch: 2/3... Step: 3300... Loss: 0.982538... Val Loss: 1.050368\n",
            "Epoch: 2/3... Step: 3400... Loss: 0.800392... Val Loss: 1.048702\n",
            "Epoch: 2/3... Step: 3500... Loss: 0.978555... Val Loss: 1.061810\n",
            "Epoch: 2/3... Step: 3600... Loss: 0.994927... Val Loss: 1.077884\n",
            "Epoch: 2/3... Step: 3700... Loss: 1.063506... Val Loss: 1.036837\n",
            "Epoch: 2/3... Step: 3800... Loss: 0.823967... Val Loss: 1.032829\n",
            "Epoch: 2/3... Step: 3900... Loss: 0.896569... Val Loss: 1.031496\n",
            "Epoch: 3/3... Step: 4000... Loss: 0.760266... Val Loss: 1.070858\n",
            "Epoch: 3/3... Step: 4100... Loss: 1.007608... Val Loss: 1.082218\n",
            "Epoch: 3/3... Step: 4200... Loss: 0.741842... Val Loss: 1.063935\n",
            "Epoch: 3/3... Step: 4300... Loss: 0.702684... Val Loss: 1.059093\n",
            "Epoch: 3/3... Step: 4400... Loss: 0.967085... Val Loss: 1.065637\n",
            "Epoch: 3/3... Step: 4500... Loss: 0.924465... Val Loss: 1.077046\n",
            "Epoch: 3/3... Step: 4600... Loss: 0.849946... Val Loss: 1.051557\n",
            "Epoch: 3/3... Step: 4700... Loss: 0.743129... Val Loss: 1.054553\n",
            "Epoch: 3/3... Step: 4800... Loss: 0.831354... Val Loss: 1.049785\n",
            "Epoch: 3/3... Step: 4900... Loss: 0.770475... Val Loss: 1.054168\n",
            "Epoch: 3/3... Step: 5000... Loss: 0.840156... Val Loss: 1.058076\n",
            "Epoch: 3/3... Step: 5100... Loss: 0.784604... Val Loss: 1.051741\n",
            "Epoch: 3/3... Step: 5200... Loss: 0.840829... Val Loss: 1.049876\n",
            "Epoch: 3/3... Step: 5300... Loss: 1.055110... Val Loss: 1.031447\n",
            "Epoch: 3/3... Step: 5400... Loss: 0.679104... Val Loss: 1.041670\n",
            "Epoch: 3/3... Step: 5500... Loss: 0.920651... Val Loss: 1.040095\n",
            "Epoch: 3/3... Step: 5600... Loss: 0.882095... Val Loss: 1.054395\n",
            "Epoch: 3/3... Step: 5700... Loss: 0.929218... Val Loss: 1.052670\n",
            "Epoch: 3/3... Step: 5800... Loss: 0.715808... Val Loss: 1.072541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22BYEdrX1ZxC",
        "colab_type": "code",
        "outputId": "1afbaa99-39c7-49f0-bdf7-303a2810d766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output, labels)\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output,1)\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.099\n",
            "Test accuracy: 0.560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOdt_GWQvj-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}